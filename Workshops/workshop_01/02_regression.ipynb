{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a3f47a-e87d-4ecf-8297-918faeadeff8",
   "metadata": {},
   "source": [
    "# Introduction to regressions with with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21a1b3-ebf7-4019-ba04-48ac1e121ff5",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: \n",
    "- Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the Andrew Whites book on [Machine learning](https://dmol.pub/ml/regression.html#exploring-effect-of-feature-number) and [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc78b1-1090-43fd-8660-b76fdbf24aab",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- How can we cast a linear regression problem as a machine learning problem\n",
    "\n",
    "**Objectives:**\n",
    "- Learn how to split data in to train, validation and test data.\n",
    "- Fit a linear regression model\n",
    "- Get an understanding of overfitting and underfitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164863f6-78d9-4fe0-99c2-a881d58ab1e7",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd895196-358c-4869-a37b-ee462f3b1aef",
   "metadata": {},
   "source": [
    "## Google Colab installs\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "The following cell install necessary packages and downloads data if you are running this tutorial using Google Colab.<br>\n",
    "<b><i>Run this cell only if you are using Google Colab!</i></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96336c11-4bda-4703-afd5-928431649a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ -n \"$COLAB_RELEASE_TAG\" ]; then git clone https://github.com/Edinburgh-Chemistry-Teaching/ATCP-ML-workshop; fi\n",
    "import os\n",
    "os.chdir(f\"ATCP-ML-workshop{os.sep}Workshops{os.sep}workshop_01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbfa9a-d7ff-4b11-b491-cfac5bf744a1",
   "metadata": {},
   "source": [
    "## 1. Introduction to regression models\n",
    "\n",
    "The objective is to train a model to learn the relation between data points. We will be looking at splitting data, learning a relationship between data and see how we can evaluate how good or bad a model has learned the data. For this purpose, we will be looking at how the loss changes over time during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533468bf-44c4-48b8-8cd8-421b037ca2f8",
   "metadata": {},
   "source": [
    "### 1.1. Using a linear model to fit data generated from a polynomial function\n",
    "\n",
    "The real function we're trying to learn will be:\n",
    "\n",
    "\\begin{equation}\n",
    " f(x) = x^3 - x^2 + x - 1\n",
    "\\end{equation}\n",
    "\n",
    "which we can rewrite as a linear model:\n",
    "\n",
    "\\begin{equation}\n",
    "  f(\\vec{x}) = \\vec{w}\\cdot\\vec{x} = [1, -1, 1, -1]\\cdot[x^3, x^2, x, 1]\n",
    "\\end{equation}\n",
    "where our features are $[x^3, x^2, x, 1]$. To do our split, we'll randomly pick 30 % of the data as test data points. To avoid the issue of convergence, we will use least squares to fit these models instead of gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece911f0-2656-45fc-a1a0-3090d948a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# generate data from polynomial\n",
    "N = 20\n",
    "syn_x = np.linspace(-3, 3, N)\n",
    "# create feature matrix these are our x values\n",
    "syn_features = np.vstack([syn_x**3, syn_x**2, syn_x, np.ones_like(syn_x)]).T\n",
    "# Here our labels are just the y values of the function\n",
    "syn_labels = syn_x**3 - syn_x**2 + syn_x - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37027101-72cb-4702-94ef-e2bcbb2c08fc",
   "metadata": {},
   "source": [
    "We have no generated model data. \n",
    "Next let's see how we can split this data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db8b63-e467-42ca-b769-8738177e98e6",
   "metadata": {},
   "source": [
    "### 1.2. Splitting data into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c199d-414e-4929-a404-2f43e51d819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224b89a-5800-4b13-b519-568af7cf38be",
   "metadata": {},
   "source": [
    "Splitting your dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into three subsets:\n",
    "\n",
    "- The training set is applied to train or fit your model. For example, you use the training set to find the optimal weights, or coefficients, for linear regression, logistic regression, or neural networks.\n",
    "\n",
    "- The validation set is used for unbiased model evaluation during hyperparameter tuning. For example, when you want to find the optimal number of neurons in a neural network or the best kernel for a support vector machine, you experiment with different values. For each considered setting of hyperparameters, you fit the model with the training set and assess its performance with the validation set.\n",
    "\n",
    "- The test set is needed for an unbiased evaluation of the final model. You shouldn’t use it for fitting or validation.\n",
    "\n",
    "In less complex cases, when you don’t have to tune hyperparameters, it’s okay to work with only the training and test sets, this is the case we will be looking at here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3346f7-ebec-43a6-8694-5ca6184c967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train/test using scikit learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b507c3e-49f9-41b3-b8e5-35c02ae4bd47",
   "metadata": {},
   "source": [
    "Now that you have imported the appropriate part from scikit learn, you can use them to split data into training sets and test sets. You’ll split inputs and outputs at the same time, with a single function call.\n",
    "\n",
    "With `train_test_split()`, you only need to provide the arrays that you want to split. Additionally, you can also provide some optional arguments. The function usually returns a list of NumPy arrays.\n",
    "\n",
    "The arrays parameter in the function signature of train_test_split() refers to the sequence of lists, NumPy arrays, pandas DataFrames, or similar array-like objects that hold the data that you want to split. All these objects together make up the dataset and must be of the same length.\n",
    "\n",
    "In supervised machine learning applications, you’ll typically work with two such arrays:\n",
    "\n",
    "1. A two-dimensional array with the inputs (x)\n",
    "2. A one-dimensional array with the outputs (y)\n",
    "The options parameter indicates that you can customize the function’s behaviour with optional keyword arguments:\n",
    "\n",
    "`train_size` is the number that defines the size of the training set. If you provide a float, then it must be between 0.0 and 1.0 and it will define the share of the dataset used for testing. If you provide an int, then it will represent the total number of the training samples. The default value is None.\n",
    "\n",
    "`test_size` is the number that defines the size of the test set. It’s very similar to train_size. You should provide either train_size or test_size. If neither is given, then the default share of the dataset that will be used for testing is 0.25, or 25 percent.\n",
    "\n",
    "`random_state` is the object that controls randomization during splitting. It can be either an int or an instance of RandomState. Setting the random state is useful if you need reproducibility. The default value is None.\n",
    "\n",
    "`shuffle` is the Boolean object that determines whether to shuffle the dataset before applying the split. The default value is True.\n",
    "\n",
    "`stratify` is an array-like object that, if not None, determines how to use a stratified split.\n",
    "\n",
    "Now it’s time to try data splitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d310bb-57bc-402d-9b1b-29a2d0dd7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(syn_features))\n",
    "x_train, x_test, y_train, y_test, train_idx, test_idx = train_test_split(\n",
    "    syn_features, syn_labels,indices, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd3ad7-98c5-43b1-9e98-e05476c45418",
   "metadata": {},
   "source": [
    "Now let's plot the data to understand what we have done in terms of splitting the data according to test and train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dd0f9-39e5-412b-9d57-22c8da4e562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting code\n",
    "figure = plt.figure(figsize=(6,6))\n",
    "plt.plot(syn_x[train_idx], y_train, \".\", label=\"Training labels\")\n",
    "plt.plot(syn_x[test_idx], y_test, \"+\", label=\"Testing labels\", color='red')\n",
    "plt.plot(syn_x, syn_labels, \"--\", label=\"Ground Truth\", color='grey')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ca4b16-acd7-4b3b-8ded-bec253115b29",
   "metadata": {},
   "source": [
    "### 1.3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70f482-5c06-45ee-b319-b245e2491dd2",
   "metadata": {},
   "source": [
    "First we will need to import the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e1668-3506-4a63-8ebe-e02452aa7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb96b7e-7750-4af6-a178-362c36992188",
   "metadata": {},
   "source": [
    "Next we will need to import the metrics that tell us how well we are doing. \n",
    "We want to use the mean square error and the coefficient of determination for this.\n",
    "Scikitlearn conveniently provides tools for this, so nothing needs to be implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ee32d-4c16-46c7-ac8b-7325e4cebb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb3e01d-a24f-409d-a058-6f5b6e8e0730",
   "metadata": {},
   "source": [
    "The `r2_score` is the coefficient of determination and tells you something about the goodness of fit. \n",
    "The `mean_square_error` is what we termed loss before and gives you information on how 'right' or 'wrong' your model is at predicting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8c375-3c96-4181-8b10-12358f2791fd",
   "metadata": {},
   "source": [
    "The steps taken to actually run a linear regression are generally always the same:\n",
    "1. Import the classes you need.\n",
    "2. Create model instances using these classes.\n",
    "3. Fit the model instances with `.fit()` using the training set.\n",
    "4. Evaluate the model with `metrics`, or directly with `.score()` using the test set.\n",
    "While here we only look at one algorithm for LinearRegressions, there are many different options in scikit learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc95be-0d72-4bb8-a17a-707786a23c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set and training set to work out the loss from the mean square error\n",
    "y_pred_test = regr.predict(x_test)\n",
    "y_pred_train = regr.predict(x_train)\n",
    "\n",
    "# The mean squared error that will give you the loss\n",
    "training_loss = mean_squared_error(syn_labels[train_idx], y_pred_train)\n",
    "test_loss = mean_squared_error(syn_labels[test_idx], y_pred_test)\n",
    "\n",
    "\n",
    "# The coefficient of determination: 1 is a perfect prediction and gives you an idea of the goodness of fit of the model\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n",
    "\n",
    "# The coefficients these are the coefficients you have successfully fit the model with\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e362e-4c45-4c86-9052-3f58284b8787",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b54787a-70bc-45ae-933c-494b1892a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e811a66-7b9d-4e0f-b720-18ceeeafc103",
   "metadata": {},
   "source": [
    "Now we have done all the fitting and work, lets see what the model has done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e96ef-6d2a-4cf2-a679-8c8c2220745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "figure = plt.figure(figsize=(6,6))\n",
    "plt.plot(syn_x[test_idx], y_test, \"^\", label=\"Testing labels truth\", color='blue', alpha=0.3)\n",
    "plt.plot(syn_x[test_idx], y_pred_test, \"+\", label=\"Testing labels prediction\", color='purple')\n",
    "plt.plot(syn_x, syn_labels, \"--\", label=\"Ground Truth\", color='grey')\n",
    "plt.plot(syn_x, np.dot(syn_features, regr.coef_), label=\"Fit Model\")\n",
    "plt.text(0, -20, f\"Training Loss: {training_loss:.5f}\")\n",
    "plt.text(0, -25, f\"Testing Loss: {test_loss:.5f}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca8e8d-21d4-496e-9225-b18292a2a4f8",
   "metadata": {},
   "source": [
    "### 1.4. Task section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0eeb8-dd83-440f-97db-3414b40937db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1: Discuss with your colleague </b> \n",
    "<p>\n",
    "    What are the assumptions made for this regression model example that make everything work really well?\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4b108-4564-49d9-8d2f-06c956efc139",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Some assumptions that were made</mark> </summary>\n",
    "\n",
    "Assumptions:\n",
    "- The data does fit a linear model\n",
    "- Random split gives a good distribution of data points\n",
    "- The number of parameters you are trying to fit indicates that you have an idea of the functional form\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4cf78b-3cac-4ad2-93cd-28f081df0015",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: How does the data splitting for training and test your prediction when you add noise to the data </b> </div>\n",
    "\n",
    "Using the same random split as before and just adding noise to the data, how does the model perform? What do you observe?\n",
    "\n",
    "You can make your model noisy in the following way \n",
    "```Python\n",
    "\n",
    "train_y = train_y + np.random.normal(scale=5, size=train_y.shape)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721b4d7-c610-4263-bcbb-806d44ad993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your solution here! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b458d4c-8f89-46b2-8546-e0376fdd7245",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "y_train_noise = y_train + np.random.normal(scale=5, size=y_train.shape)\n",
    "\n",
    "# Let's plot the data with the random noise\n",
    "figure = plt.figure(figsize=(6,6))\n",
    "plt.plot(syn_x[train_idx], y_train_noise, \".\", label=\"Training labels\")\n",
    "plt.plot(syn_x[test_idx], y_test, \"+\", label=\"Testing labels\", color='red')\n",
    "plt.plot(syn_x, syn_labels, \"--\", label=\"Ground Truth\", color='grey')\n",
    "plt.legend()\n",
    "\n",
    "# Now let's do the training again\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train_noise)\n",
    "\n",
    "# Make predictions using the testing set and training set to work out the loss from the mean square error\n",
    "y_pred_test = regr.predict(x_test)\n",
    "y_pred_train = regr.predict(x_train)\n",
    "\n",
    "# The mean squared error that will give you the loss\n",
    "training_loss = mean_squared_error(syn_labels[train_idx], y_pred_train)\n",
    "test_loss = mean_squared_error(syn_labels[test_idx], y_pred_test)\n",
    "\n",
    "\n",
    "# The coefficient of determination: 1 is a perfect prediction and gives you an idea of the goodness of fit of the model\n",
    "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred_test))\n",
    "\n",
    "# The coefficients these are the coefficients you have successfully fit the model with\n",
    "print(\"Coefficients: \\n\", regr.coef_)\n",
    "\n",
    "# Plot outputs\n",
    "figure = plt.figure(figsize=(6,6))\n",
    "plt.plot(syn_x[test_idx], y_test, \"^\", label=\"Testing labels truth\", color='blue', alpha=0.3)\n",
    "plt.plot(syn_x[test_idx], y_pred_test, \"+\", label=\"Testing labels prediction\", color='purple')\n",
    "plt.plot(syn_x, np.dot(syn_features, regr.coef_), label=\"Fit Model\")\n",
    "plt.plot(syn_x, syn_labels, \"--\", label=\"Ground Truth\", color='grey')\n",
    "plt.text(0, -20, f\"Training Loss: {training_loss:.2f}\")\n",
    "plt.text(0, -25, f\"Testing Loss: {test_loss:.2f}\")\n",
    "plt.legend()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c4765-2d08-425a-ba66-ee51fd344212",
   "metadata": {},
   "source": [
    "## 2. Overfitting and underfitting your data\n",
    "\n",
    "Splitting a dataset might also be important for detecting if your model suffers from one of two very common problems, called underfitting and overfitting:\n",
    "\n",
    "- Underfitting is usually the consequence of a model being unable to encapsulate the relations among data. For example, this can happen when trying to represent nonlinear relations with a linear model. Underfitted models will likely have poor performance with both training and test sets.\n",
    "\n",
    "- Overfitting usually takes place when a model has an excessively complex structure and learns both the existing relations among data and noise. Such models often have bad generalization capabilities. Although they work well with training data, they usually yield poor performance with unseen test data.\n",
    "\n",
    "You can find a more detailed explanation of underfitting and overfitting in [Linear Regression](https://realpython.com/linear-regression-in-python/#underfitting-and-overfitting) in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc64da5-e622-4842-afe6-28dbffca6c26",
   "metadata": {},
   "source": [
    "## 2.1 Task section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413de0a-3038-41af-8a06-8577610794ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1: Take a look at how you split your data: </b> \n",
    "</div>\n",
    "\n",
    "- Try splitting using only the first 6 data points for training\n",
    "- Try using only the first 6 and last 6 data points for training   \n",
    "What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3a3bc-fa1a-43af-8691-88776a0c3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
